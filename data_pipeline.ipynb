{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import spacy\n",
    "from spacy.training import offsets_to_biluo_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"ai4privacy/pii-masking-300k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_eng_ind_val = 7946\n",
    "last_eng_ind_train = 29908"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "document_ids = dataset['validation'][:last_eng_ind_val]['id']\n",
    "full_texts = dataset['validation'][:last_eng_ind_val]['source_text']\n",
    "span_labels = dataset['validation'][:last_eng_ind_val]['span_labels']\n",
    "\n",
    "test_dataset = [{'id':document_ids[i][:-1], 'full_text': full_texts[i], 'span_label': literal_eval(span_labels[i])} for i in range(last_eng_ind_val)]\n",
    "\n",
    "document_ids = dataset['train'][:last_eng_ind_train]['id']\n",
    "full_texts = dataset['train'][:last_eng_ind_train]['source_text']\n",
    "span_labels = dataset['train'][:last_eng_ind_train]['span_labels']\n",
    "\n",
    "train_dataset = [{'id':document_ids[i][:-1], 'full_text': full_texts[i], 'span_label': literal_eval(span_labels[i])} for i in range(last_eng_ind_train)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "all_labels = ['B-STREET',\n",
    " 'B-CITY',\n",
    " 'I-DATE',\n",
    " 'B-PASS',\n",
    " 'I-CITY',\n",
    " 'B-TIME',\n",
    " 'B-EMAIL',\n",
    " 'I-DRIVERLICENSE',\n",
    " 'I-POSTCODE',\n",
    " 'I-BOD',\n",
    " 'B-USERNAME',\n",
    " 'B-BOD',\n",
    " 'B-COUNTRY',\n",
    " 'B-SECADDRESS',\n",
    " 'B-IDCARD',\n",
    " 'I-SOCIALNUMBER',\n",
    " 'I-PASSPORT',\n",
    " 'B-IP',\n",
    " 'O',\n",
    " 'B-TEL',\n",
    " 'B-SOCIALNUMBER',\n",
    " 'I-TIME',\n",
    " 'B-BUILDING',\n",
    " 'B-PASSPORT',\n",
    " 'I-TITLE',\n",
    " 'I-SEX',\n",
    " 'I-STREET',\n",
    " 'B-STATE',\n",
    " 'I-STATE',\n",
    " 'B-TITLE',\n",
    " 'B-DATE',\n",
    " 'B-GEOCOORD',\n",
    " 'I-IDCARD',\n",
    " 'I-TEL',\n",
    " 'B-POSTCODE',\n",
    " 'B-DRIVERLICENSE',\n",
    " 'I-GEOCOORD',\n",
    " 'I-COUNTRY',\n",
    " 'I-EMAIL',\n",
    " 'I-PASS',\n",
    " 'B-SEX',\n",
    " 'I-USERNAME',\n",
    " 'I-BUILDING',\n",
    " 'I-IP',\n",
    " 'I-SECADDRESS',\n",
    " 'B-CARDISSUER',\n",
    " 'I-CARDISSUER']\n",
    "id2label = {i: l for i, l in enumerate(all_labels)}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "target = [l for l in all_labels if l != \"O\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "len(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "unique_labels = set()\n",
    "for i in all_labels:\n",
    "    unique_labels.add(i[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60996a6f2c19486d9d53b232d4a1f2b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfd467f822f24b5a90663a2b659ceced",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f23b62c97e6457bb9ebdc7d1a796462",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/760k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ecfd1adc07b43b89950d77ae33c1b1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "deberta_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v2-xlarge\")\n",
    "distilbert_tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "albert_tokenizer = AutoTokenizer.from_pretrained(\"albert/albert-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def biluo_to_bio(tags):\n",
    "    new_tags = []\n",
    "    for i in tags:\n",
    "        if i.startswith(\"U-\"):\n",
    "            new_tags.append(i.replace(\"U-\", \"B-\"))\n",
    "            continue\n",
    "        if i.startswith(\"L-\"):\n",
    "            new_tags.append(i.replace(\"L-\", \"I-\"))\n",
    "            continue\n",
    "        new_tags.append(i)\n",
    "    return new_tags\n",
    "\n",
    "def create_bio_labels(text, spans, tokenizer):\n",
    "    doc = nlp(text)\n",
    "    spacy_tokens = [token.text for token in doc]\n",
    "    spacy_tags = offsets_to_biluo_tags(doc, spans)\n",
    "    if '-' in spacy_tags:\n",
    "        raise Exception(\"error\")\n",
    "    bio_tags = biluo_to_bio(spacy_tags)\n",
    "    tokenized_input = tokenizer(spacy_tokens, is_split_into_words=True, truncation=True)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "    word_ids = tokenized_input.word_ids()\n",
    "    tokenized_bio = []\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            tokenized_bio.append(-100)\n",
    "        elif word_idx != previous_word_idx:  # Only label the first token of a given words\n",
    "            if bio_tags[word_idx][-1] in ['1', '2', '3']:\n",
    "                tokenized_bio.append(label2id[bio_tags[word_idx][:-1]])\n",
    "            else:\n",
    "                tokenized_bio.append(label2id[bio_tags[word_idx]])\n",
    "        else:\n",
    "            tokenized_bio.append(-100)\n",
    "        previous_word_idx = word_idx\n",
    "    return tokens, tokenized_input[\"input_ids\"], tokenized_bio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m tokenized_dataset \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      2\u001b[0m total_wrong \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 3\u001b[0m dataset \u001b[38;5;241m=\u001b[39m test_dataset\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m dataset:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = []\n",
    "total_wrong = 0\n",
    "dataset = test_dataset\n",
    "for i in dataset:\n",
    "    try:\n",
    "        tokens, token_ids, tokenized_bio = create_bio_labels(i['full_text'], i['span_label'], albert_tokenizer)\n",
    "        tokenized_dataset.append({\n",
    "            'id': i['id'],\n",
    "            'tokens': tokens,\n",
    "            'token_ids': token_ids,\n",
    "            'bio_labels': tokenized_bio,\n",
    "            'source_text': i['full_text']\n",
    "        })\n",
    "    except:\n",
    "        total_wrong += 1\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "modelname = 'ensemble'\n",
    "train_test = 'test'\n",
    "\n",
    "with open(f\"{modelname}_{train_test}.json\", \"w\") as f:\n",
    "    f.write(json.dumps(tokenized_dataset))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
