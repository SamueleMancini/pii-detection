{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "import aux\n",
    "from tqdm import tqdm\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps in epoch:   0%|          | 0/18244 [00:00<?, ?it/s]/Users/andreafabbricatore/Desktop/mac_bocconi/year 3/semester 2/AI/project/aux.py:195: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  stacked_tensors1 = torch.stack([torch.tensor(i) for i in output1])\n",
      "/Users/andreafabbricatore/Desktop/mac_bocconi/year 3/semester 2/AI/project/aux.py:198: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  stacked_tensors2 = torch.stack([torch.tensor(i) for i in output2])\n",
      "Steps in epoch: 100%|██████████| 18244/18244 [38:26<00:00,  7.91it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.2237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps in epoch: 100%|██████████| 18244/18244 [40:15<00:00,  7.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Loss: 0.2237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps in epoch: 100%|██████████| 18244/18244 [42:33<00:00,  7.14it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Loss: 0.2237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps in epoch: 100%|██████████| 18244/18244 [42:36<00:00,  7.14it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Loss: 0.2237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps in epoch: 100%|██████████| 18244/18244 [43:26<00:00,  7.00it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Loss: 0.2237\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "from aux import ensembler, json_to_Dataset_ensemble\n",
    "from tqdm import tqdm\n",
    "\n",
    "class KingBert(nn.Module):\n",
    "    def __init__(self, distilbert_tuned, albert_tuned):\n",
    "        super().__init__()\n",
    "        self.distilbert = distilbert_tuned\n",
    "        self.albert = albert_tuned\n",
    "\n",
    "        for distilbert_param in self.distilbert.parameters():\n",
    "            distilbert_param.requires_grad = False\n",
    "\n",
    "        for albert_param in self.albert.parameters():\n",
    "            albert_param.requires_grad = False \n",
    "        \n",
    "        self.alpha = nn.Parameter(0.5 * torch.ones(47), requires_grad=True)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, distilbert_input_ids, albert_input_ids, distil_attention_mask, alb_attention_mask, distilbert_word_ids, albert_word_ids):\n",
    "        distilbert_output = self.distilbert(input_ids=distilbert_input_ids, attention_mask=distil_attention_mask)\n",
    "        albert_output = self.albert(input_ids=albert_input_ids, attention_mask=alb_attention_mask)\n",
    "        distilbert_fixed, albert_fixed = aux.ensembler(distilbert_output['logits'].squeeze(), albert_output['logits'].squeeze(), distilbert_word_ids.squeeze(), albert_word_ids.squeeze())\n",
    "\n",
    "        distilbert_fixed = self.softmax(distilbert_fixed)\n",
    "        albert_fixed = self.softmax(albert_fixed)\n",
    "\n",
    "        final_output = distilbert_fixed * self.alpha + albert_fixed * (torch.ones(47) - self.alpha)\n",
    "\n",
    "        return self.softmax(final_output)\n",
    "\n",
    "train_dataset = aux.json_to_Dataset_ensemble('data/ensemble_train.json')\n",
    "\n",
    "distilbert_tuned = AutoModelForTokenClassification.from_pretrained('distilbert_finetuned')\n",
    "albert_tuned = AutoModelForTokenClassification.from_pretrained('albert_finetuned')\n",
    "\n",
    "kingbert_model = KingBert(distilbert_tuned, albert_tuned)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(kingbert_model.parameters(), lr=2e-5)\n",
    "\n",
    "\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for i in tqdm(range(len(train_dataset)), desc=\"Steps in epoch\"):\n",
    "        try:\n",
    "\n",
    "            item = train_dataset[i]\n",
    "            \n",
    "            distilbert_input_ids = torch.tensor(item['distilbert_inputids']).unsqueeze(0)\n",
    "            albert_input_ids = torch.tensor(item['albert_inputids']).unsqueeze(0)\n",
    "            distil_attention_mask = torch.tensor(item['distilbert_attention_masks']).unsqueeze(0)\n",
    "            alb_attention_mask = torch.tensor(item['albert_attention_masks']).unsqueeze(0)\n",
    "            distilbert_word_ids = torch.tensor([-100] + item['distilbert_wordids'][1:-1] + [-100]).unsqueeze(0)\n",
    "            albert_word_ids = torch.tensor([-100] + item['albert_wordids'][1:-1] + [-100]).unsqueeze(0)\n",
    "            targets = torch.tensor(item['spacy_labels']).unsqueeze(0)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = kingbert_model(distilbert_input_ids, albert_input_ids, distil_attention_mask, alb_attention_mask, distilbert_word_ids, albert_word_ids)\n",
    "            \n",
    "            ohe_targets = torch.zeros(output.shape[0], output.shape[1])\n",
    "            for i,j in enumerate(targets):\n",
    "                ohe_targets[i][j] = 1\n",
    "\n",
    "            loss = criterion(output, ohe_targets)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "    avg_loss = total_loss / len(train_dataset)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
    "\n",
    "torch.save(kingbert_model.state_dict(), 'model_state.pth')\n",
    "\n",
    "print('Training complete.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
