{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'auxiliary'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mauxiliary\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01maux\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      3\u001b[0m     AutoModelForTokenClassification,\n\u001b[1;32m      4\u001b[0m     TrainingArguments,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m     DataCollatorForTokenClassification,\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'auxiliary'"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForTokenClassification,\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from seqeval.metrics import classification_report\n",
    "from nlp_project.models import CubeBert\n",
    "from nlp_project.data import json_to_Dataset, json_to_Dataset_adv, json_to_Dataset_ensemble\n",
    "from nlp_project.utils import compute_metrics, inference\n",
    "\n",
    "all_labels = [\n",
    "    \"B-STREET\",\n",
    "    \"B-CITY\",\n",
    "    \"I-DATE\",\n",
    "    \"B-PASS\",\n",
    "    \"I-CITY\",\n",
    "    \"B-TIME\",\n",
    "    \"B-EMAIL\",\n",
    "    \"I-DRIVERLICENSE\",\n",
    "    \"I-POSTCODE\",\n",
    "    \"I-BOD\",\n",
    "    \"B-USERNAME\",\n",
    "    \"B-BOD\",\n",
    "    \"B-COUNTRY\",\n",
    "    \"B-SECADDRESS\",\n",
    "    \"B-IDCARD\",\n",
    "    \"I-SOCIALNUMBER\",\n",
    "    \"I-PASSPORT\",\n",
    "    \"B-IP\",\n",
    "    \"O\",\n",
    "    \"B-TEL\",\n",
    "    \"B-SOCIALNUMBER\",\n",
    "    \"I-TIME\",\n",
    "    \"B-BUILDING\",\n",
    "    \"B-PASSPORT\",\n",
    "    \"I-TITLE\",\n",
    "    \"I-SEX\",\n",
    "    \"I-STREET\",\n",
    "    \"B-STATE\",\n",
    "    \"I-STATE\",\n",
    "    \"B-TITLE\",\n",
    "    \"B-DATE\",\n",
    "    \"B-GEOCOORD\",\n",
    "    \"I-IDCARD\",\n",
    "    \"I-TEL\",\n",
    "    \"B-POSTCODE\",\n",
    "    \"B-DRIVERLICENSE\",\n",
    "    \"I-GEOCOORD\",\n",
    "    \"I-COUNTRY\",\n",
    "    \"I-EMAIL\",\n",
    "    \"I-PASS\",\n",
    "    \"B-SEX\",\n",
    "    \"I-USERNAME\",\n",
    "    \"I-BUILDING\",\n",
    "    \"I-IP\",\n",
    "    \"I-SECADDRESS\",\n",
    "    \"B-CARDISSUER\",\n",
    "    \"I-CARDISSUER\",\n",
    "]\n",
    "\n",
    "id2label = {i: l for i, l in enumerate(all_labels)}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "n_labels = len(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_all_metrics(model, tokenizer, dataset, batch_size=8):\n",
    "    model = model.to(\"cuda\")\n",
    "    model.eval()\n",
    "\n",
    "    # Drop non-numeric columns\n",
    "    dataset = dataset.remove_columns([\"source_text\", \"tokens\"])\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=DataCollatorForTokenClassification(\n",
    "            tokenizer, return_tensors=\"pt\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    all_predictions_text = []\n",
    "    all_labels_text = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch[\"input_ids\"].to(\"cuda\")\n",
    "            attention_mask = batch[\"attention_mask\"].to(\"cuda\")\n",
    "            labels = batch[\"labels\"]\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "            labels = labels.numpy()\n",
    "\n",
    "            all_predictions.extend(preds)\n",
    "            all_labels.extend(labels)\n",
    "\n",
    "            for pred_seq, label_seq in zip(preds, labels):\n",
    "                pred_labels = []\n",
    "                true_labels = []\n",
    "                for p, l in zip(pred_seq, label_seq):\n",
    "                    if l == -100:\n",
    "                        continue\n",
    "                    pred_labels.append(id2label[p])\n",
    "                    true_labels.append(id2label[l])\n",
    "                all_predictions_text.append(pred_labels)\n",
    "                all_labels_text.append(true_labels)\n",
    "\n",
    "    print(classification_report(all_labels_text, all_predictions_text))\n",
    "\n",
    "    return compute_metrics(all_predictions, all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_test = json_to_Dataset(\"data/distilbert_test.json\")\n",
    "a_test = json_to_Dataset(\"data/albert_test.json\")\n",
    "d_val = json_to_Dataset(\"data/distilbert_val.json\")\n",
    "a_val = json_to_Dataset(\"data/albert_val.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_d1_model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"to_share/distilbert1\"\n",
    ")\n",
    "old_d1_tokenizer = AutoTokenizer.from_pretrained(\"to_share/distilbert1\")\n",
    "new_d1_model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"models/distilbert1\"\n",
    ")\n",
    "new_d1_tokenizer = AutoTokenizer.from_pretrained(\"models/distilbert1\")\n",
    "\n",
    "old_d2_model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"to_share/distilbert2\"\n",
    ")\n",
    "old_d2_tokenizer = AutoTokenizer.from_pretrained(\"to_share/distilbert2\")\n",
    "new_d2_model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"models/distilbert2\"\n",
    ")\n",
    "new_d2_tokenizer = AutoTokenizer.from_pretrained(\"models/distilbert2\")\n",
    "\n",
    "old_a_model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"to_share/albert1\"\n",
    ")\n",
    "old_a_tokenizer = AutoTokenizer.from_pretrained(\"to_share/albert1\")\n",
    "new_a_model = AutoModelForTokenClassification.from_pretrained(\"models/albert1\")\n",
    "new_a_tokenizer = AutoTokenizer.from_pretrained(\"models/albert1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_test = json_to_Dataset_adv(\"data/distilbert_test_adv.json\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"models/distilbert1\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"models/distilbert1\")\n",
    "res = compute_all_metrics(model, d_test)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = json_to_Dataset_adv(\"data/albert_test_adv.json\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"models/albert1\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"models/albert1\")\n",
    "res = compute_all_metrics(model, test)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res1_old = compute_all_metrics(old_d1_model, old_d1_tokenizer, d_test)\n",
    "res1_new = compute_all_metrics(new_d1_model, new_d1_tokenizer, d_test)\n",
    "res2_old = compute_all_metrics(old_d2_model, old_d2_tokenizer, d_test)\n",
    "res2_new = compute_all_metrics(new_d2_model, new_d2_tokenizer, d_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res1_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res1_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res2_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res2_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resa_old = compute_all_metrics(old_a_model, old_a_tokenizer, a_test)\n",
    "resa_new = compute_all_metrics(new_a_model, new_a_tokenizer, a_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resa_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resa_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res1_new_val = compute_all_metrics(new_d1_model, new_d1_tokenizer, d_val)\n",
    "res2_new_val = compute_all_metrics(new_d2_model, new_d2_tokenizer, d_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res1_new_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res2_new_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resa_new_val = compute_all_metrics(new_a_model, new_a_tokenizer, a_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resa_new_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resac_new_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = np.log(res[\"confusion_matrix\"] + 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf, cmap=\"Reds\")\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title(\"Confusion Matrix Distilbert Finetuned 1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a test phrase with special tokens\n",
    "\n",
    "test_phrase = \"My name is Cubo, and my credit card is 4111-1111-1111-1111 issued by VISA.\"\n",
    "test_phrase = test_phrase\n",
    "\n",
    "\n",
    "# Tokenize the input\n",
    "inputs = tokenizer.tokenize(test_phrase)\n",
    "input_ids = torch.tensor(\n",
    "    [[101] + tokenizer.convert_tokens_to_ids(inputs) + [102]]\n",
    ")\n",
    "attention_mask = torch.tensor([1 for i in range(len(input_ids))]).unsqueeze(0)\n",
    "\n",
    "print(input_ids)\n",
    "print(attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, predictions, predicted_token_class, inputs = inference(\n",
    "    model, input_ids, attention_mask\n",
    ")\n",
    "\n",
    "print(predicted_token_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the tokens from the tokenizer\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "\n",
    "# Print results\n",
    "print(\"Token\\tPrediction\")\n",
    "print(\"-\" * 30)\n",
    "for token, prediction in zip(tokens, predicted_token_class):\n",
    "    print(f\"{token}\\t{prediction}\")\n",
    "\n",
    "# Visualize the results with color coding\n",
    "colored_text = []\n",
    "current_entity = None\n",
    "for token, label in zip(tokens, predicted_token_class):\n",
    "    # Skip special tokens\n",
    "    if token in [\n",
    "        tokenizer.cls_token,\n",
    "        tokenizer.sep_token,\n",
    "        tokenizer.pad_token,\n",
    "    ]:\n",
    "        continue\n",
    "\n",
    "    # Handle subword tokens (starting with ##)\n",
    "    token_display = token.replace(\"##\", \"\")\n",
    "\n",
    "    if label == \"O\":\n",
    "        # Not an entity\n",
    "        colored_text.append(token_display)\n",
    "        current_entity = None\n",
    "    elif label.startswith(\"B-\"):\n",
    "        # Beginning of entity\n",
    "        entity_type = label[2:]\n",
    "        colored_text.append(f\"\\033[1m\\033[91m{token_display}\\033[0m\")\n",
    "        current_entity = entity_type\n",
    "    elif label.startswith(\"I-\"):\n",
    "        # Inside an entity\n",
    "        entity_type = label[2:]\n",
    "        colored_text.append(f\"\\033[1m\\033[91m{token_display}\\033[0m\")\n",
    "        current_entity = entity_type\n",
    "\n",
    "# Join tokens to form text (this is simplified and might not be perfect for all tokenizers)\n",
    "reconstructed_text = \"\".join(colored_text).replace(\" ##\", \"\")\n",
    "print(\"\\nColored text (PII in red):\")\n",
    "print(reconstructed_text)\n",
    "\n",
    "# Create a more readable visualization\n",
    "print(\"\\nDetected PII entities:\")\n",
    "entity_spans = []\n",
    "current_entity = None\n",
    "current_start = None\n",
    "\n",
    "for i, (token, label) in enumerate(zip(tokens, predicted_token_class)):\n",
    "    # Skip special tokens\n",
    "    if token in [\n",
    "        tokenizer.cls_token,\n",
    "        tokenizer.sep_token,\n",
    "        tokenizer.pad_token,\n",
    "    ]:\n",
    "        continue\n",
    "\n",
    "    if label.startswith(\"B-\"):\n",
    "        # Beginning of a new entity\n",
    "        if current_entity:\n",
    "            # Save the previous entity\n",
    "            entity_spans.append((current_start, i - 1, current_entity))\n",
    "        current_entity = label[2:]\n",
    "        current_start = i\n",
    "    elif label.startswith(\"I-\"):\n",
    "        # Inside an entity - continue\n",
    "        pass\n",
    "    elif label == \"O\":\n",
    "        # Outside any entity\n",
    "        if current_entity:\n",
    "            # Save the previous entity\n",
    "            entity_spans.append((current_start, i - 1, current_entity))\n",
    "            current_entity = None\n",
    "\n",
    "# Add the last entity if there is one\n",
    "if current_entity:\n",
    "    entity_spans.append((current_start, len(tokens) - 1, current_entity))\n",
    "\n",
    "# Print the original text with highlighted entities\n",
    "original_tokens = tokenizer.convert_tokens_to_string(tokens).split()\n",
    "for start, end, entity_type in entity_spans:\n",
    "    entity_text = \" \".join(original_tokens[start : end + 1])\n",
    "    print(f\"{entity_type}: {entity_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"distilbert_finetuned2\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert_finetuned2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = compute_all_metrics(model, data)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = np.log(res[\"confusion_matrix\"] + 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf, cmap=\"Reds\")\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title(\"Confusion Matrix Distilbert Finetuned 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = json_to_Dataset(\"data/albert_test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [i for i in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\"albert_finetuned\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"albert_finetuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = aux.compute_all_metrics(model, data)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = np.log(res[\"confusion_matrix\"] + 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf, cmap=\"Reds\")\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title(\"Confusion Matrix Albert Finetuned 1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\"albert_finetuned2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"albert_finetuned2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = compute_all_metrics(model, data)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = np.log(res[\"confusion_matrix\"] + 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf, cmap=\"Reds\")\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title(\"Confusion Matrix Albert Finetuned 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "distilbert_tuned = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"distilbert_finetuned\"\n",
    ")\n",
    "albert_tuned = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"albert_finetuned\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CuboBert(distilbert_tuned=distilbert_tuned, albert_tuned=albert_tuned)\n",
    "state_dict = torch.load(\"model_state.pth\")\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = json_to_Dataset_ensemble(\"data/ensemble_test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [i for i in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = compute_ensemble_metrics(model, data)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = np.log(res[\"confusion_matrix\"] + 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf, cmap=\"Reds\")\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title(\"Confusion Matrix KingBERT\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
